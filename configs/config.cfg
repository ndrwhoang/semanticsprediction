[general]
mode=train
device_id=0
use_gpu=True
seed=69420

[dataloading]
use_pretrained_tokenizer=False
offset_value = 5
vocab_limit=20000
max_len=512

[model]
max_len=512
vocab_size=50265
d_embedding=512
n_layer=2
d_model=512
n_head=8
dim_feedforward=2048
dropout=0.3
n_node=44
n_edge=18

[training]
n_epoch=10
bsz_train = 8
bsz_val = 8
save_every_n_epoch=5

[data_path]
vocab=data\auxiliary\vocab.json
label_vocab=data\auxiliary\label_vocab.json
vocab_dict=data\auxiliary\s_token2id.json
label_vocab_dict=data\auxiliary\l_token2id.json

train_raw=data\raw\sample_train_seq2seq.json
val_raw=data\raw\sample_train_seq2seq.json
test_raw=data\raw\sample_train_seq2seq.json