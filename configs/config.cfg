[general]
mode=train
device_id=0
use_gpu=True
num_worker=4
seed=123

[dataloading]
use_pretrained_tokenizer=False
offset_value = 5
vocab_limit=20000
max_len=512
null_value=10

[model]
roberta_version=distilroberta-base
freeze_pretrained=True
max_len=512
vocab_size=50265
d_embedding=768
n_layer=2
d_model=768
n_head=8
dim_feedforward=2048
dropout=0.2
n_node=44
n_edge=18

[training]
node_subspace = factuality genericity time entity_type
edge_subspace = none
lr = 0.002
unfreeze_lr = 0.00002
lr_finetune = 0.00002
max_lr=0.02
n_epoch=1
bsz_train = 8
bsz_val = 8
bsz_finetune = 8
bsz_test=8
save_every_n_epoch=5
finetune_thresh=0.5
mixed_precision=True
grad_accumulation=True
grad_accumulation_steps=4

[data_path]
vocab=data\auxiliary\vocab.json
label_vocab=data\auxiliary\label_vocab.json
vocab_dict=data\auxiliary\s_token2id.json
label_vocab_dict=data\auxiliary\l_token2id.json
node_dict=data\auxiliary\node_token2id.json
edge_dict=data\auxiliary\edge_token2id.json
subspace_dict=data\auxiliary\subspace2id.json

train_raw=data\raw\train_seq2seq.json
val_raw=data\raw\val_seq2seq.json
test_raw=data\raw\test_seq2seq.json
train_subset_raw=data\raw\sample_train_seq2seq.json

train_entity_type=data\processed\train_entity_type_seq2seq.json
train_factuality=data\processed\train_factuality_seq2seq.json
train_genericity=data\processed\train_genericity_seq2seq.json
train_protoroles=data\processed\train_protoroles_seq2seq.json
train_time=data\processed\train_time_seq2seq.json

[model_path]
checkpoint_dir=checkpoints\
encoder_base_ckpt=checkpoints\node_only.pt
node_only=checkpoints\node_only.pt