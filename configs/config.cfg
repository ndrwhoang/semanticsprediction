[general]
mode=train
device_id=0
use_gpu=True
seed=69420

[dataloading]
use_pretrained_tokenizer=False
offset_value = 5
vocab_limit=20000


[model]
vocab_size=20000
embedding_dim=512
n_layer=2
d_model=512
n_head=8
dim_feedforward=2048
dropout=0.3


[training]
n_epoch=10
bsz_train = 8
bsz_val = 8
save_every_n_epoch=5

[data_path]
vocab=data\auxiliary\vocab.json
label_vocab=data\auxiliary\label_vocab.json
vocab_dict=data\auxiliary\s_token2id.json
label_vocab_dict=data\auxiliary\l_token2id.json

train_raw=data\raw\sample_train.json
val_raw=data\raw\sample_train.json
test_raw=data\raw\sample_train.json