[general]
mode=train
device_id=0
use_gpu=True
seed=123

[dataloading]
use_pretrained_tokenizer=False
offset_value = 5
vocab_limit=20000
max_len=512
null_value=10

[model]
roberta_version=distilroberta-base
freeze_pretrained=True
max_len=512
vocab_size=50265
d_embedding=768
n_layer=2
d_model=768
n_head=8
dim_feedforward=2048
dropout=0.2
n_node=44
n_edge=18

[training]
node_subspace = genericity
edge_subspace = none
lr = 0.002
unfreeze_lr = 0.00002
max_lr=0.02
n_epoch=15
bsz_train = 32
bsz_val = 16
bsz_test=8
save_every_n_epoch=5
finetune_thresh=0.5

[data_path]
vocab=data\auxiliary\vocab.json
label_vocab=data\auxiliary\label_vocab.json
vocab_dict=data\auxiliary\s_token2id.json
label_vocab_dict=data\auxiliary\l_token2id.json
node_dict=data\auxiliary\node_token2id.json
edge_dict=data\auxiliary\edge_token2id.json
subspace_dict=data\auxiliary\subspace2id.json

train_raw=data\raw\train_seq2seq.json
val_raw=data\raw\val_seq2seq.json
test_raw=data\raw\test_seq2seq.json
train_subset_raw=data\raw\sample_train_seq2seq.json

[model_path]
checkpoint_dir=checkpoints\